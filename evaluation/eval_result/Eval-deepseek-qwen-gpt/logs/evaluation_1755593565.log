2025-08-19 14:22:45,052 - INFO - Logging initialized. Log file: /Users/rish/Development/runpod dev/llm-tests/evaluation/eval_result/Eval-deepseek-qwen-gpt/logs/evaluation_1755593565.log
2025-08-19 14:22:45,052 - INFO - Starting medical notes evaluation
2025-08-19 14:22:45,052 - INFO - Configuration:
2025-08-19 14:22:45,052 - INFO -   - Benchmark directory: benchmark
2025-08-19 14:22:45,052 - INFO -   - Candidate directory: ../Output/Deepseek-qwen32b-fp8-updatedTemp
2025-08-19 14:22:45,053 - INFO -   - Transcript directory: ../transcripts/Spencer
2025-08-19 14:22:45,053 - INFO -   - Output directory: /Users/rish/Development/runpod dev/llm-tests/evaluation/eval_result/Eval-deepseek-qwen-gpt
2025-08-19 14:22:45,053 - INFO -   - Model: gpt-5
2025-08-19 14:22:45,053 - INFO - Created output directory: /Users/rish/Development/runpod dev/llm-tests/evaluation/eval_result/Eval-deepseek-qwen-gpt
2025-08-19 14:22:45,129 - INFO - OpenAI model 'gpt-5' initialized successfully
2025-08-19 14:22:45,129 - INFO - Loading evaluation prompt from: eval_prompt.txt
2025-08-19 14:22:45,130 - INFO - Loaded prompt (2738 characters)
2025-08-19 14:22:45,130 - INFO - Found 20 benchmark files to process
2025-08-19 14:22:45,130 - INFO - Processing 1/20: REC-6627
2025-08-19 14:22:47,511 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-19 14:22:47,515 - WARNING - OpenAI API call failed (attempt 1/3): Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}
2025-08-19 14:22:49,736 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-19 14:22:49,736 - WARNING - OpenAI API call failed (attempt 2/3): Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}
2025-08-19 14:22:53,169 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-19 14:22:53,170 - WARNING - OpenAI API call failed (attempt 3/3): Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}
2025-08-19 14:22:53,170 - ERROR - All OpenAI API attempts failed for this request
2025-08-19 14:22:53,170 - ERROR - ‚ùå Error evaluating REC-6627: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}
Traceback (most recent call last):
  File "/Users/rish/Development/runpod dev/llm-tests/evaluation/eval.py", line 288, in main
    response_text = call_openai_with_retry(client, full_prompt, logger)
  File "/Users/rish/Development/runpod dev/llm-tests/evaluation/eval.py", line 168, in call_openai_with_retry
    response = client.chat.completions.create(
        model=MODEL_NAME,
    ...<4 lines>...
        temperature=0,  # deterministic output
    )
  File "/Users/rish/Development/runpod dev/llm-tests/evaluation/eval-venv/lib/python3.13/site-packages/openai/_utils/_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "/Users/rish/Development/runpod dev/llm-tests/evaluation/eval-venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1153, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<47 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/rish/Development/runpod dev/llm-tests/evaluation/eval-venv/lib/python3.13/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/rish/Development/runpod dev/llm-tests/evaluation/eval-venv/lib/python3.13/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported value: 'temperature' does not support 0 with this model. Only the default (1) value is supported.", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}
2025-08-19 14:22:53,182 - INFO - Processing 2/20: REC-6607
