Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.
Your new public key is: 

ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIGJrBmRuL9g63Gbu5LT3qHiWUpkse3h4u7qu2CvKm6x/

time=2025-08-20T14:00:43.962Z level=INFO source=routes.go:1318 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NEW_ESTIMATES:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-08-20T14:00:43.962Z level=INFO source=images.go:477 msg="total blobs: 0"
time=2025-08-20T14:00:43.963Z level=INFO source=images.go:484 msg="total unused blobs removed: 0"
time=2025-08-20T14:00:43.963Z level=INFO source=routes.go:1371 msg="Listening on 127.0.0.1:11434 (version 0.11.5)"
time=2025-08-20T14:00:43.963Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-08-20T14:00:44.613Z level=INFO source=types.go:130 msg="inference compute" id=GPU-cf93252c-36aa-8687-30d6-d3d0ee02104b library=cuda variant=v12 compute=8.6 driver=12.7 name="NVIDIA A40" total="44.4 GiB" available="44.2 GiB"
time=2025-08-20T14:00:44.613Z level=INFO source=types.go:130 msg="inference compute" id=GPU-186565ee-b225-7eb2-fa77-905be8a3cf90 library=cuda variant=v12 compute=8.6 driver=12.7 name="NVIDIA A40" total="44.4 GiB" available="44.2 GiB"
[GIN] 2025/08/20 - 14:03:19 | 200 |      47.977µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/20 - 14:03:19 | 404 |     523.807µs |       127.0.0.1 | POST     "/api/show"
time=2025-08-20T14:03:20.726Z level=INFO source=download.go:177 msg="downloading 07ca3450446e in 55 1 GB part(s)"
time=2025-08-20T14:04:59.073Z level=INFO source=download.go:177 msg="downloading e0a42594d802 in 1 358 B part(s)"
time=2025-08-20T14:05:00.441Z level=INFO source=download.go:177 msg="downloading dd084c7d92a3 in 1 8.4 KB part(s)"
time=2025-08-20T14:05:01.771Z level=INFO source=download.go:177 msg="downloading 3116c5225075 in 1 77 B part(s)"
time=2025-08-20T14:05:03.160Z level=INFO source=download.go:177 msg="downloading fe29a560051c in 1 487 B part(s)"
[GIN] 2025/08/20 - 14:05:52 | 200 |         2m32s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/20 - 14:05:52 | 200 |  315.816595ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-20T14:05:54.550Z level=INFO source=server.go:383 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /root/.ollama/models/blobs/sha256-07ca3450446e07c4e3dfd55d34e3f426963a15f1db00c3093d9214c202d12e25 --port 37561"
time=2025-08-20T14:05:54.570Z level=INFO source=runner.go:1006 msg="starting ollama engine"
time=2025-08-20T14:05:54.570Z level=INFO source=runner.go:1043 msg="Server listening on 127.0.0.1:37561"
time=2025-08-20T14:05:54.935Z level=INFO source=server.go:488 msg="system memory" total="503.5 GiB" free="433.3 GiB" free_swap="0 B"
time=2025-08-20T14:05:54.939Z level=INFO source=memory.go:36 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=/root/.ollama/models/blobs/sha256-07ca3450446e07c4e3dfd55d34e3f426963a15f1db00c3093d9214c202d12e25 library=cuda parallel=1 required="58.6 GiB" gpus=2
time=2025-08-20T14:05:54.942Z level=INFO source=server.go:531 msg=offload library=cuda layers.requested=-1 layers.model=63 layers.offload=63 layers.split="[32 31]" memory.available="[44.2 GiB 44.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="58.6 GiB" memory.required.partial="58.6 GiB" memory.required.kv="944.0 MiB" memory.required.allocations="[31.4 GiB 27.2 GiB]" memory.weights.total="50.3 GiB" memory.weights.repeating="47.7 GiB" memory.weights.nonrepeating="2.6 GiB" memory.graph.full="1.6 GiB" memory.graph.partial="1.6 GiB" projector.weights="795.9 MiB" projector.graph="1.0 GiB"
time=2025-08-20T14:05:54.943Z level=INFO source=runner.go:925 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:48 GPULayers:63[ID:GPU-cf93252c-36aa-8687-30d6-d3d0ee02104b Layers:32(0..31) ID:GPU-186565ee-b225-7eb2-fa77-905be8a3cf90 Layers:31(32..62)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-08-20T14:05:55.086Z level=INFO source=ggml.go:130 msg="" architecture=gemma3 file_type=F16 name="" description="" num_tensors=1247 num_key_values=37
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 2 CUDA devices:
  Device 0: NVIDIA A40, compute capability 8.6, VMM: yes, ID: GPU-cf93252c-36aa-8687-30d6-d3d0ee02104b
  Device 1: NVIDIA A40, compute capability 8.6, VMM: yes, ID: GPU-186565ee-b225-7eb2-fa77-905be8a3cf90
load_backend: loaded CUDA backend from /usr/local/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-08-20T14:05:55.324Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 CUDA.1.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.1.USE_GRAPHS=1 CUDA.1.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-08-20T14:05:55.824Z level=INFO source=ggml.go:486 msg="offloading 62 repeating layers to GPU"
time=2025-08-20T14:05:55.825Z level=INFO source=ggml.go:492 msg="offloading output layer to GPU"
time=2025-08-20T14:05:55.825Z level=INFO source=ggml.go:497 msg="offloaded 63/63 layers to GPU"
time=2025-08-20T14:05:55.826Z level=INFO source=backend.go:310 msg="model weights" device=CUDA0 size="24.6 GiB"
time=2025-08-20T14:05:55.826Z level=INFO source=backend.go:310 msg="model weights" device=CUDA1 size="26.5 GiB"
time=2025-08-20T14:05:55.826Z level=INFO source=backend.go:315 msg="model weights" device=CPU size="2.6 GiB"
time=2025-08-20T14:05:55.826Z level=INFO source=backend.go:321 msg="kv cache" device=CUDA0 size="484.0 MiB"
time=2025-08-20T14:05:55.826Z level=INFO source=backend.go:321 msg="kv cache" device=CUDA1 size="460.0 MiB"
time=2025-08-20T14:05:55.827Z level=INFO source=backend.go:332 msg="compute graph" device=CUDA0 size="301.3 MiB"
time=2025-08-20T14:05:55.827Z level=INFO source=backend.go:332 msg="compute graph" device=CUDA1 size="1.1 GiB"
time=2025-08-20T14:05:55.827Z level=INFO source=backend.go:337 msg="compute graph" device=CPU size="10.5 MiB"
time=2025-08-20T14:05:55.827Z level=INFO source=backend.go:342 msg="total memory" size="56.1 GiB"
time=2025-08-20T14:05:55.827Z level=INFO source=sched.go:473 msg="loaded runners" count=1
time=2025-08-20T14:05:55.827Z level=INFO source=server.go:1234 msg="waiting for llama runner to start responding"
time=2025-08-20T14:05:55.828Z level=INFO source=server.go:1268 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-20T14:06:12.422Z level=INFO source=server.go:1272 msg="llama runner started in 17.87 seconds"
[GIN] 2025/08/20 - 14:06:12 | 200 | 19.510826204s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/20 - 14:06:18 | 200 |    1.110184ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/08/20 - 14:08:08 | 200 |         1m50s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/20 - 14:10:10 | 200 |         1m59s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/20 - 14:11:08 | 500 | 55.669929016s |       127.0.0.1 | POST     "/api/generate"
