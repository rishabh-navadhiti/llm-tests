Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.
Your new public key is: 

ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBVnCv5gGQb2yTFLi6iunDZAaA0PTrj+NtmYbByGS9vE

time=2025-08-07T11:27:43.247Z level=INFO source=routes.go:1297 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-08-07T11:27:43.248Z level=INFO source=images.go:477 msg="total blobs: 0"
time=2025-08-07T11:27:43.248Z level=INFO source=images.go:484 msg="total unused blobs removed: 0"
time=2025-08-07T11:27:43.248Z level=INFO source=routes.go:1350 msg="Listening on 127.0.0.1:11434 (version 0.11.3)"
time=2025-08-07T11:27:43.248Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-08-07T11:27:43.589Z level=INFO source=types.go:130 msg="inference compute" id=GPU-a41332a8-48fb-7752-d9c8-e63d9805d78c library=cuda variant=v12 compute=8.6 driver=12.7 name="NVIDIA RTX A5000" total="23.6 GiB" available="23.4 GiB"
[GIN] 2025/08/07 - 11:28:09 | 200 |      82.852µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/08/07 - 11:28:09 | 404 |     684.497µs |       127.0.0.1 | POST     "/api/show"
time=2025-08-07T11:28:09.822Z level=INFO source=download.go:177 msg="downloading b112e727c6f1 in 16 861 MB part(s)"
time=2025-08-07T11:28:36.178Z level=INFO source=download.go:177 msg="downloading 51468a0fd901 in 1 7.4 KB part(s)"
time=2025-08-07T11:28:37.531Z level=INFO source=download.go:177 msg="downloading f60356777647 in 1 11 KB part(s)"
time=2025-08-07T11:28:38.852Z level=INFO source=download.go:177 msg="downloading d8ba2f9a17b3 in 1 18 B part(s)"
time=2025-08-07T11:28:40.199Z level=INFO source=download.go:177 msg="downloading 8d6fddaf04b2 in 1 489 B part(s)"
[GIN] 2025/08/07 - 11:28:54 | 200 | 45.683472521s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/08/07 - 11:28:54 | 200 |  205.993916ms |       127.0.0.1 | POST     "/api/show"
time=2025-08-07T11:28:55.557Z level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/root/.ollama/models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-a41332a8-48fb-7752-d9c8-e63d9805d78c parallel=1 available=25089605632 required="14.9 GiB"
time=2025-08-07T11:28:55.788Z level=INFO source=server.go:135 msg="system memory" total="503.5 GiB" free="473.8 GiB" free_swap="0 B"
time=2025-08-07T11:28:55.789Z level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[23.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-08-07T11:28:55.923Z level=INFO source=server.go:438 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /root/.ollama/models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 48 --parallel 1 --port 35373"
time=2025-08-07T11:28:55.926Z level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-08-07T11:28:55.927Z level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-08-07T11:28:55.927Z level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-07T11:28:55.943Z level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-08-07T11:28:55.944Z level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:35373"
time=2025-08-07T11:28:56.028Z level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA RTX A5000, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-08-07T11:28:56.137Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-08-07T11:28:56.180Z level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-07T11:28:56.274Z level=INFO source=ggml.go:367 msg="offloading 24 repeating layers to GPU"
time=2025-08-07T11:28:56.274Z level=INFO source=ggml.go:373 msg="offloading output layer to GPU"
time=2025-08-07T11:28:56.274Z level=INFO source=ggml.go:378 msg="offloaded 25/25 layers to GPU"
time=2025-08-07T11:28:56.274Z level=INFO source=ggml.go:381 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-08-07T11:28:56.274Z level=INFO source=ggml.go:381 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-08-07T11:28:56.285Z level=INFO source=ggml.go:672 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-08-07T11:28:56.285Z level=INFO source=ggml.go:672 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-08-07T11:29:05.243Z level=INFO source=server.go:637 msg="llama runner started in 9.32 seconds"
[GIN] 2025/08/07 - 11:29:05 | 200 | 10.328446503s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/07 - 11:31:16 | 200 |         1m47s |       127.0.0.1 | POST     "/api/generate"
time=2025-08-07T11:48:29.844Z level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/root/.ollama/models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-a41332a8-48fb-7752-d9c8-e63d9805d78c parallel=1 available=25089605632 required="14.9 GiB"
time=2025-08-07T11:48:30.096Z level=INFO source=server.go:135 msg="system memory" total="503.5 GiB" free="474.2 GiB" free_swap="0 B"
time=2025-08-07T11:48:30.097Z level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[23.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-08-07T11:48:30.208Z level=INFO source=server.go:438 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /root/.ollama/models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 48 --parallel 1 --port 35851"
time=2025-08-07T11:48:30.211Z level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-08-07T11:48:30.212Z level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-08-07T11:48:30.213Z level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-07T11:48:30.232Z level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-08-07T11:48:30.233Z level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:35851"
time=2025-08-07T11:48:30.326Z level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA RTX A5000, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-08-07T11:48:30.436Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-08-07T11:48:30.466Z level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-07T11:48:30.578Z level=INFO source=ggml.go:367 msg="offloading 24 repeating layers to GPU"
time=2025-08-07T11:48:30.578Z level=INFO source=ggml.go:373 msg="offloading output layer to GPU"
time=2025-08-07T11:48:30.578Z level=INFO source=ggml.go:378 msg="offloaded 25/25 layers to GPU"
time=2025-08-07T11:48:30.578Z level=INFO source=ggml.go:381 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-08-07T11:48:30.578Z level=INFO source=ggml.go:381 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-08-07T11:48:30.593Z level=INFO source=ggml.go:672 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-08-07T11:48:30.593Z level=INFO source=ggml.go:672 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-08-07T11:48:40.032Z level=INFO source=server.go:637 msg="llama runner started in 9.82 seconds"
[GIN] 2025/08/07 - 11:50:23 | 200 |         1m54s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/07 - 11:51:22 | 200 | 29.914652055s |       127.0.0.1 | POST     "/api/generate"
time=2025-08-07T12:02:55.694Z level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/root/.ollama/models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-a41332a8-48fb-7752-d9c8-e63d9805d78c parallel=1 available=25089605632 required="14.9 GiB"
time=2025-08-07T12:02:55.945Z level=INFO source=server.go:135 msg="system memory" total="503.5 GiB" free="461.9 GiB" free_swap="0 B"
time=2025-08-07T12:02:55.945Z level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[23.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-08-07T12:02:56.045Z level=INFO source=server.go:438 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /root/.ollama/models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 48 --parallel 1 --port 38549"
time=2025-08-07T12:02:56.046Z level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-08-07T12:02:56.046Z level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-08-07T12:02:56.047Z level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-07T12:02:56.062Z level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-08-07T12:02:56.063Z level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:38549"
time=2025-08-07T12:02:56.148Z level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA RTX A5000, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-08-07T12:02:56.241Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-08-07T12:02:56.299Z level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-07T12:02:56.357Z level=INFO source=ggml.go:367 msg="offloading 24 repeating layers to GPU"
time=2025-08-07T12:02:56.357Z level=INFO source=ggml.go:373 msg="offloading output layer to GPU"
time=2025-08-07T12:02:56.357Z level=INFO source=ggml.go:378 msg="offloaded 25/25 layers to GPU"
time=2025-08-07T12:02:56.357Z level=INFO source=ggml.go:381 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-08-07T12:02:56.357Z level=INFO source=ggml.go:381 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-08-07T12:02:56.368Z level=INFO source=ggml.go:672 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-08-07T12:02:56.368Z level=INFO source=ggml.go:672 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-08-07T12:03:00.066Z level=INFO source=server.go:637 msg="llama runner started in 4.02 seconds"
[GIN] 2025/08/07 - 12:03:30 | 200 | 35.482617775s |       127.0.0.1 | POST     "/api/generate"
time=2025-08-07T12:17:20.264Z level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/root/.ollama/models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-a41332a8-48fb-7752-d9c8-e63d9805d78c parallel=1 available=25089605632 required="14.9 GiB"
time=2025-08-07T12:17:20.514Z level=INFO source=server.go:135 msg="system memory" total="503.5 GiB" free="474.4 GiB" free_swap="0 B"
time=2025-08-07T12:17:20.515Z level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[23.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-08-07T12:17:20.612Z level=INFO source=server.go:438 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /root/.ollama/models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 48 --parallel 1 --port 37843"
time=2025-08-07T12:17:20.614Z level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-08-07T12:17:20.614Z level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-08-07T12:17:20.615Z level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-07T12:17:20.636Z level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-08-07T12:17:20.637Z level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:37843"
time=2025-08-07T12:17:20.728Z level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA RTX A5000, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-08-07T12:17:20.834Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-08-07T12:17:20.868Z level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-07T12:17:20.978Z level=INFO source=ggml.go:367 msg="offloading 24 repeating layers to GPU"
time=2025-08-07T12:17:20.979Z level=INFO source=ggml.go:373 msg="offloading output layer to GPU"
time=2025-08-07T12:17:20.979Z level=INFO source=ggml.go:378 msg="offloaded 25/25 layers to GPU"
time=2025-08-07T12:17:20.979Z level=INFO source=ggml.go:381 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-08-07T12:17:20.979Z level=INFO source=ggml.go:381 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-08-07T12:17:20.993Z level=INFO source=ggml.go:672 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-08-07T12:17:20.993Z level=INFO source=ggml.go:672 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-08-07T12:17:21.622Z level=WARN source=server.go:605 msg="client connection closed before server finished loading, aborting load"
time=2025-08-07T12:17:21.623Z level=ERROR source=sched.go:487 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
[GIN] 2025/08/07 - 12:17:21 | 499 |  2.023667855s |       127.0.0.1 | POST     "/api/generate"
time=2025-08-07T12:17:25.607Z level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/root/.ollama/models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-a41332a8-48fb-7752-d9c8-e63d9805d78c parallel=1 available=25089605632 required="14.9 GiB"
time=2025-08-07T12:17:25.840Z level=INFO source=server.go:135 msg="system memory" total="503.5 GiB" free="474.1 GiB" free_swap="0 B"
time=2025-08-07T12:17:25.841Z level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[23.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-08-07T12:17:25.964Z level=INFO source=server.go:438 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /root/.ollama/models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 48 --parallel 1 --port 37481"
time=2025-08-07T12:17:25.967Z level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-08-07T12:17:25.968Z level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-08-07T12:17:25.969Z level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-07T12:17:25.989Z level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-08-07T12:17:25.990Z level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:37481"
time=2025-08-07T12:17:26.081Z level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA RTX A5000, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-08-07T12:17:26.196Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-08-07T12:17:26.221Z level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-07T12:17:26.347Z level=INFO source=ggml.go:367 msg="offloading 24 repeating layers to GPU"
time=2025-08-07T12:17:26.347Z level=INFO source=ggml.go:373 msg="offloading output layer to GPU"
time=2025-08-07T12:17:26.347Z level=INFO source=ggml.go:378 msg="offloaded 25/25 layers to GPU"
time=2025-08-07T12:17:26.347Z level=INFO source=ggml.go:381 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-08-07T12:17:26.347Z level=INFO source=ggml.go:381 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-08-07T12:17:26.362Z level=INFO source=ggml.go:672 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-08-07T12:17:26.362Z level=INFO source=ggml.go:672 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-08-07T12:17:35.534Z level=INFO source=server.go:637 msg="llama runner started in 9.57 seconds"
[GIN] 2025/08/07 - 12:18:08 | 200 | 43.678975046s |       127.0.0.1 | POST     "/api/generate"
time=2025-08-07T12:28:53.305Z level=INFO source=sched.go:786 msg="new model will fit in available VRAM in single GPU, loading" model=/root/.ollama/models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 gpu=GPU-a41332a8-48fb-7752-d9c8-e63d9805d78c parallel=1 available=25089605632 required="14.9 GiB"
time=2025-08-07T12:28:53.561Z level=INFO source=server.go:135 msg="system memory" total="503.5 GiB" free="474.1 GiB" free_swap="0 B"
time=2025-08-07T12:28:53.562Z level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split="" memory.available="[23.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="14.9 GiB" memory.required.partial="14.9 GiB" memory.required.kv="300.0 MiB" memory.required.allocations="[14.9 GiB]" memory.weights.total="11.7 GiB" memory.weights.repeating="10.7 GiB" memory.weights.nonrepeating="1.1 GiB" memory.graph.full="2.0 GiB" memory.graph.partial="2.0 GiB"
time=2025-08-07T12:28:53.655Z level=INFO source=server.go:438 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /root/.ollama/models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --ctx-size 8192 --batch-size 512 --n-gpu-layers 25 --threads 48 --parallel 1 --port 37403"
time=2025-08-07T12:28:53.658Z level=INFO source=sched.go:481 msg="loaded runners" count=1
time=2025-08-07T12:28:53.658Z level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2025-08-07T12:28:53.659Z level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server not responding"
time=2025-08-07T12:28:53.678Z level=INFO source=runner.go:925 msg="starting ollama engine"
time=2025-08-07T12:28:53.679Z level=INFO source=runner.go:983 msg="Server listening on 127.0.0.1:37403"
time=2025-08-07T12:28:53.770Z level=INFO source=ggml.go:92 msg="" architecture=gptoss file_type=MXFP4 name="" description="" num_tensors=315 num_key_values=30
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA RTX A5000, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-08-07T12:28:53.878Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-08-07T12:28:53.912Z level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server loading model"
time=2025-08-07T12:28:54.024Z level=INFO source=ggml.go:367 msg="offloading 24 repeating layers to GPU"
time=2025-08-07T12:28:54.024Z level=INFO source=ggml.go:373 msg="offloading output layer to GPU"
time=2025-08-07T12:28:54.024Z level=INFO source=ggml.go:378 msg="offloaded 25/25 layers to GPU"
time=2025-08-07T12:28:54.024Z level=INFO source=ggml.go:381 msg="model weights" buffer=CUDA0 size="11.7 GiB"
time=2025-08-07T12:28:54.024Z level=INFO source=ggml.go:381 msg="model weights" buffer=CPU size="1.1 GiB"
time=2025-08-07T12:28:54.038Z level=INFO source=ggml.go:672 msg="compute graph" backend=CUDA0 buffer_type=CUDA0 size="2.1 GiB"
time=2025-08-07T12:28:54.038Z level=INFO source=ggml.go:672 msg="compute graph" backend=CPU buffer_type=CPU size="5.6 MiB"
time=2025-08-07T12:29:03.228Z level=INFO source=server.go:637 msg="llama runner started in 9.57 seconds"
[GIN] 2025/08/07 - 12:29:52 | 200 | 59.397578057s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/08/07 - 12:33:00 | 200 |         1m24s |       127.0.0.1 | POST     "/api/generate"
